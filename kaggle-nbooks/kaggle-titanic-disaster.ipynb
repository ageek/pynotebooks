{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cat /home/bakuda/oldUbunuHome/bakuda/sandbox/ageekrepo/kaggle/titanic-disaster/myfirstforest.py\n",
      "\"\"\" Writing my first randomforest code.\n",
      "Author : AstroDave\n",
      "Date : 23rd September, 2012\n",
      "please see packages.python.org/milk/randomforests.html for more\n",
      "\n",
      "\"\"\" \n",
      "\n",
      "import numpy as np\n",
      "import csv as csv\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "csv_file_object = csv.reader(open('/home/bakuda/oldUbunuHome/bakuda/sandbox/ageekrepo/kaggle/titanic-disaster/train.csv', 'rb')) #Load in the training csv file\n",
      "header = csv_file_object.next() #Skip the fist line as it is a header\n",
      "train_data=[] #Creat a variable called 'train_data'\n",
      "for row in csv_file_object: #Skip through each row in the csv file\n",
      "    train_data.append(row[1:]) #adding each row to the data variable\n",
      "train_data = np.array(train_data) #Then convert from a list to an array\n",
      "\n",
      "print train_data.shape\n",
      "\n",
      "#I need to convert all strings to integer classifiers:\n",
      "#Male = 1, female = 0:\n",
      "train_data[train_data[0::,3]=='male',3] = 1\n",
      "train_data[train_data[0::,3]=='female',3] = 0\n",
      "#embark c=0, s=1, q=2\n",
      "train_data[train_data[0::,10] =='C',10] = 0\n",
      "train_data[train_data[0::,10] =='S',10] = 1\n",
      "train_data[train_data[0::,10] =='Q',10] = 2\n",
      "\n",
      "#I need to fill in the gaps of the data and make it complete.\n",
      "#So where there is no price, I will assume price on median of that class\n",
      "#Where there is no age I will give median of all ages\n",
      "\n",
      "#All the ages with no data make the median of the data\n",
      "train_data[train_data[0::,4] == '',4] = np.median(train_data[train_data[0::,4]\\\n",
      "                                           != '',4].astype(np.float))\n",
      "#All missing ebmbarks just make them embark from most common place\n",
      "train_data[train_data[0::,10] == '',10] = np.round(np.mean(train_data[train_data[0::,10]\\\n",
      "                                                   != '',10].astype(np.float)))\n",
      "\n",
      "train_data = np.delete(train_data,[2,7,9],1) #remove the name data, cabin and ticket\n",
      "#I need to do the same with the test data now so that the columns are in the same\n",
      "#as the training data\n",
      "\n",
      "test_file_object = csv.reader(open('/home/bakuda/oldUbunuHome/bakuda/sandbox/ageekrepo/kaggle/titanic-disaster/test.csv', 'rb')) #Load in the test csv file\n",
      "header = test_file_object.next() #Skip the fist line as it is a header\n",
      "test_data=[] #Creat a variable called 'test_data'\n",
      "ids = []\n",
      "for row in test_file_object: #Skip through each row in the csv file\n",
      "    ids.append(row[0])\n",
      "    test_data.append(row[1:]) #adding each row to the data variable\n",
      "test_data = np.array(test_data) #Then convert from a list to an array\n",
      "\n",
      "print test_data.shape\n",
      "\n",
      "#I need to convert all strings to integer classifiers:\n",
      "#Male = 1, female = 0:\n",
      "test_data[test_data[0::,2]=='male',2] = 1\n",
      "test_data[test_data[0::,2]=='female',2] = 0\n",
      "#ebark c=0, s=1, q=2\n",
      "test_data[test_data[0::,9] =='C',9] = 0 #Note this is not ideal, in more complex 3 is not 3 tmes better than 1 than 2 is 2 times better than 1\n",
      "test_data[test_data[0::,9] =='S',9] = 1\n",
      "test_data[test_data[0::,9] =='Q',9] = 2\n",
      "\n",
      "#All the ages with no data make the median of the data\n",
      "test_data[test_data[0::,3] == '',3] = np.median(test_data[test_data[0::,3]\\\n",
      "                                           != '',3].astype(np.float))\n",
      "#All missing ebmbarks just make them embark from most common place\n",
      "test_data[test_data[0::,9] == '',9] = np.round(np.mean(test_data[test_data[0::,9]\\\n",
      "                                                   != '',9].astype(np.float)))\n",
      "#All the missing prices assume median of their respectice class\n",
      "for i in xrange(np.size(test_data[0::,0])):\n",
      "    if test_data[i,7] == '':\n",
      "        test_data[i,7] = np.median(test_data[(test_data[0::,7] != '') &\\\n",
      "                                             (test_data[0::,0] == test_data[i,0])\\\n",
      "            ,7].astype(np.float))\n",
      "\n",
      "test_data = np.delete(test_data,[1,6,8],1) #remove the name data, cabin and ticket\n",
      "\n",
      "#The data is now ready to go. So lets train then test!\n",
      "\n",
      "print 'Training '\n",
      "forest = RandomForestClassifier(n_estimators=100)\n",
      "\n",
      "forest = forest.fit(train_data[0::,1::],\\\n",
      "                    train_data[0::,0])\n",
      "\n",
      "print 'Predicting'\n",
      "output = forest.predict(test_data)\n",
      "\n",
      "open_file_object = csv.writer(open(\"/home/bakuda/ageekrepo/kaggle/titanic-disaster/myfirstforest.csv\", \"wb\"))\n",
      "open_file_object.writerow([\"PassengerId\",\"Survived\"])\n",
      "open_file_object.writerows(zip(ids, output))\n",
      "print \"done writing prediction file\"\n",
      "#np.savetxt(\"/home/bakuda/ageekrepo/kaggle/titanic-disaster/prediction.csv\", output, delimiter='\\n', fmt=\"%s\")\n",
      "#np.savetxt(\"/home/bakuda/ageekrepo/kaggle/titanic-disaster/id.csv\",ids, delimiter='\\n', fmt=\"%d\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(891, 11)\n",
        "(418, 10)\n",
        "Training \n",
        "Predicting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done writing prediction file\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['0' '0' '0' '1' '1' '0' '0' '0' '1' '0' '0' '0' '1' '0' '1' '1' '0' '1'\n",
        " '1' '0' '1' '1' '1' '1' '1' '0' '1' '1' '1' '0' '0' '0' '1' '0' '1' '1'\n",
        " '0' '0' '0' '1' '0' '1' '0' '1' '1' '0' '0' '0' '1' '1' '1' '0' '1' '1'\n",
        " '0' '0' '0' '0' '0' '1' '0' '0' '0' '1' '1' '1' '1' '0' '0' '1' '1' '0'\n",
        " '0' '0' '1' '0' '0' '1' '0' '1' '1' '0' '0' '0' '0' '0' '1' '0' '1' '1'\n",
        " '0' '1' '1' '0' '0' '0' '1' '0' '1' '0' '1' '0' '0' '0' '1' '0' '0' '0'\n",
        " '1' '0' '0' '1' '1' '1' '1' '0' '0' '1' '0' '1' '1' '0' '1' '0' '0' '1'\n",
        " '0' '1' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0'\n",
        " '1' '0' '0' '0' '1' '0' '1' '0' '0' '1' '0' '0' '1' '1' '1' '1' '1' '1'\n",
        " '1' '1' '0' '1' '0' '0' '1' '0' '0' '0' '0' '0' '0' '1' '1' '1' '1' '1'\n",
        " '0' '0' '1' '0' '1' '0' '1' '0' '0' '0' '0' '0' '1' '0' '1' '0' '1' '0'\n",
        " '0' '0' '1' '1' '0' '1' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0'\n",
        " '1' '0' '1' '0' '1' '0' '1' '1' '1' '0' '0' '1' '0' '0' '0' '1' '0' '0'\n",
        " '1' '0' '0' '1' '1' '1' '1' '1' '0' '0' '0' '0' '1' '0' '1' '0' '1' '0'\n",
        " '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '1' '1' '0' '0' '0' '0' '0' '0'\n",
        " '0' '0' '1' '1' '0' '1' '0' '0' '0' '0' '0' '1' '1' '0' '1' '0' '0' '0'\n",
        " '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '1' '1'\n",
        " '1' '1' '0' '0' '0' '1' '0' '0' '1' '1' '0' '0' '0' '0' '0' '0' '0' '1'\n",
        " '1' '0' '1' '0' '0' '0' '1' '0' '0' '1' '0' '0' '1' '0' '0' '0' '0' '0'\n",
        " '0' '1' '0' '1' '0' '1' '0' '1' '1' '0' '0' '0' '1' '0' '1' '0' '0' '1'\n",
        " '0' '1' '1' '1' '1' '0' '0' '0' '1' '1' '0' '1' '0' '0' '1' '1' '0' '0'\n",
        " '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '1'\n",
        " '0' '1' '0' '0' '1' '0' '1' '0' '0' '1' '0' '0' '1' '1' '1' '1' '0' '0'\n",
        " '1' '0' '0' '1']\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del train_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array(['0', '3', '1', '22', '1', '0', '7.25', '1'], \n",
        "      dtype='|S82')"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}