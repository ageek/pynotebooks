{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Ref: http://easymachinelearning.blogspot.in/p/sparse-auto-encoders.html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"Sparse Autoencoder\n",
      "\"\"\"\n",
      "\n",
      "# Author: Issam Laradji <issam.laradji@gmail.com>\n",
      "\n",
      "import numpy as np\n",
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn.utils import atleast2d_or_csr, check_random_state\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "\n",
      "def _binary_KL_divergence(p, p_hat):\n",
      "    return (p * np.log(p / p_hat)) + ((1 - p) * np.log((1 - p) / (1 - p_hat)))\n",
      "\n",
      "def _logistic(X):\n",
      "    return 1. / (1. + np.exp(np.clip(-X, -30, 30)))\n",
      "    \n",
      "def _d_logistic(X):\n",
      "    return X * (1 - X)\n",
      "    \n",
      "class Autoencoder(BaseEstimator, TransformerMixin):\n",
      "    def __init__(\n",
      "        self, n_hidden=25,\n",
      "        learning_rate=0.3, alpha=3e-3, beta=3, sparsity_param=0.1,\n",
      "        max_iter=20, verbose=False, random_state=None):\n",
      "        self.n_hidden = n_hidden\n",
      "        self.alpha = alpha\n",
      "        self.learning_rate = learning_rate\n",
      "        self.beta = beta\n",
      "        self.sparsity_param = sparsity_param\n",
      "        self.max_iter = max_iter\n",
      "        self.verbose = verbose\n",
      "        self.random_state = random_state\n",
      "\n",
      "    def _init_fit(self, n_features):\n",
      "        \"\"\"\n",
      "        Weights' initilization\n",
      "        \"\"\"\n",
      "        rng = check_random_state(self.random_state)\n",
      "        self.coef_hidden_ = rng.uniform(-1, 1, (n_features, self.n_hidden))\n",
      "        self.coef_output_ = rng.uniform(-1, 1, (self.n_hidden, n_features))\n",
      "        self.intercept_hidden_ = rng.uniform(-1, 1, self.n_hidden)\n",
      "        self.intercept_output_ = rng.uniform(-1, 1, n_features)\n",
      "\n",
      "    def _unpack(self, theta, n_features):\n",
      "        N = self.n_hidden * n_features\n",
      "        self.coef_hidden_ = np.reshape(theta[:N],\n",
      "                                      (n_features, self.n_hidden))\n",
      "        self.coef_output_ = np.reshape(theta[N:2 * N],\n",
      "                                      (self.n_hidden, n_features))\n",
      "        self.intercept_hidden_ = theta[2 * N:2 * N + self.n_hidden]\n",
      "        self.intercept_output_ = theta[2 * N + self.n_hidden:]\n",
      "\n",
      "    def _pack(self, W1, W2, b1, b2):\n",
      "        return np.hstack((W1.ravel(), W2.ravel(),\n",
      "                          b1.ravel(), b2.ravel()))\n",
      "\n",
      "    def transform(self, X):\n",
      "        return _logistic(np.dot(X, self.coef_hidden_) + self.intercept_hidden_)\n",
      "\n",
      "    def fit_transform(self, X, y=None):\n",
      "        self.fit(X)\n",
      "        return self.transform(X)\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        n_samples, n_features = X.shape\n",
      "        self._init_fit(n_features)\n",
      "        self._backprop_lbfgs(\n",
      "                X, n_features, n_samples)\n",
      "        return self\n",
      "\n",
      "    def backprop(self, X, n_features, n_samples):\n",
      "        # Forward pass\n",
      "        a_hidden = _logistic(np.dot(X, self.coef_hidden_)\n",
      "                                      + self.intercept_hidden_)\n",
      "        a_output = _logistic(np.dot(a_hidden, self.coef_output_)\n",
      "                                      + self.intercept_output_)\n",
      "        # Compute average activation of hidden neurons\n",
      "        p = self.sparsity_param\n",
      "        p_hat = np.mean(a_hidden, 0)\n",
      "        p_delta  = self.beta * ((1 - p) / (1 - p_hat) - p / p_hat)\n",
      "        # Compute cost \n",
      "        diff = X - a_output\n",
      "        cost = np.sum(diff ** 2) / (2 * n_samples)\n",
      "        # Add regularization term to cost \n",
      "        cost += (0.5 * self.alpha) * (\n",
      "            np.sum(self.coef_hidden_ ** 2) + np.sum(\n",
      "                self.coef_output_ ** 2))\n",
      "        # Add sparsity term to the cost\n",
      "        cost += self.beta * np.sum(\n",
      "            _binary_KL_divergence(\n",
      "                p, p_hat))\n",
      "        # Compute the error terms (delta)\n",
      "        delta_output = -diff * _d_logistic(a_output)\n",
      "        delta_hidden = (\n",
      "            (np.dot(delta_output, self.coef_output_.T) +\n",
      "             p_delta)) * _d_logistic(a_hidden)\n",
      "        #Get gradients\n",
      "        W1grad = np.dot(X.T, delta_hidden) / n_samples \n",
      "        W2grad = np.dot(a_hidden.T, delta_output) / n_samples\n",
      "        b1grad = np.mean(delta_hidden, 0) \n",
      "        b2grad = np.mean(delta_output, 0) \n",
      "        # Add regularization term to weight gradients \n",
      "        W1grad += self.alpha * self.coef_hidden_\n",
      "        W2grad += self.alpha * self.coef_output_\n",
      "        return cost, W1grad, W2grad, b1grad, b2grad\n",
      "\n",
      "    def _backprop_lbfgs(self, X, n_features, n_samples):\n",
      "        #Pack the initial weights \n",
      "        #into a vector\n",
      "        initial_theta = self._pack(\n",
      "            self.coef_hidden_,\n",
      "            self.coef_output_,\n",
      "            self.intercept_hidden_,\n",
      "            self.intercept_output_)\n",
      "        #Optimize the weights using l-bfgs\n",
      "        optTheta, _, _ = fmin_l_bfgs_b(\n",
      "            func=self._cost_grad,\n",
      "            x0=initial_theta,\n",
      "            maxfun=self.max_iter,\n",
      "            disp=self.verbose,\n",
      "            args=(X,\n",
      "                n_features,\n",
      "                n_samples))\n",
      "        #Unpack the weights into their\n",
      "        #relevant variables\n",
      "        self._unpack(optTheta, n_features)\n",
      "\n",
      "    def _cost_grad(self, theta, X, n_features,\n",
      "                   n_samples):\n",
      "        self._unpack(theta, n_features)\n",
      "        cost, W1grad, W2grad, b1grad, b2grad = self.backprop(\n",
      "            X, n_features, n_samples)\n",
      "        return cost, self._pack(W1grad, W2grad, b1grad, b2grad)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    from sklearn.linear_model import SGDClassifier\n",
      "    from sklearn.datasets import fetch_mldata\n",
      "    import random\n",
      "    #Download dataset\n",
      "    mnist = fetch_mldata('MNIST original')\n",
      "    #Get 1000 samples for 'X', and 'y'\n",
      "    X, y = mnist.data, mnist.target\n",
      "    random.seed(1)\n",
      "    indices = np.array(random.sample(range(70000), 1000))\n",
      "    X, y = X[indices].astype('float64'), y[indices]\n",
      "    # Scale values in the range [0, 1]\n",
      "    X = X / 255\n",
      "    # Set autoencoder parameters\n",
      "    ae = Autoencoder(max_iter=200,sparsity_param=0.1,\n",
      "                                    beta=3,n_hidden = 190,alpha=3e-3,\n",
      "                                    verbose=True, random_state=1)\n",
      "    # Train and extract features\n",
      "    extracted_features = ae.fit_transform(X)\n",
      "    # Test\n",
      "    clf = SGDClassifier(random_state=3)\n",
      "    clf.fit(X, y)\n",
      "    print 'SGD on raw pixels score: ', \\\n",
      "              clf.score(X, y)\n",
      "    clf.fit(extracted_features, y)\n",
      "    print 'SGD on extracted features score: ', \\\n",
      "              clf.score(extracted_features, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SGD on raw pixels score:  0.938\n",
        "SGD on extracted features score:  0.976\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extracted_features.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "(1000, 190)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.shape, y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "((1000, 784), (1000,))"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import classification_report, confusion_matrix, auc_score\n",
      "clf_rf = RandomForestClassifier()\n",
      "scores = cross_val_score(clf_rf, X,y, cv=5)\n",
      "print scores, scores.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.855  0.755  0.77   0.785  0.795] 0.792\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_rf = RandomForestClassifier()\n",
      "scores = cross_val_score(clf_rf, extracted_features,y, cv=5)\n",
      "print scores, scores.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.865  0.82   0.8    0.78   0.785] 0.81\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_sgd=SGDClassifier()\n",
      "scores = cross_val_score(clf_sgd, X,y, cv=5)\n",
      "print scores, scores.mean()\n",
      "\n",
      "scores = cross_val_score(clf_sgd, extracted_features,y, cv=5)\n",
      "print scores, scores.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.875  0.785  0.805  0.82   0.825] 0.822\n",
        "[ 0.895  0.87   0.88   0.88   0.91 ]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.887\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}